model:
  d_model: 128 #256
  n_heads: 2
  num_encoder_layers: 2 #4
  num_decoder_layers: 2 #4
  d_ff: 512 #1024
  dropout: 0.1
  max_seq_length: 256
  vocab_size: 30000

training:
  batch_size: 64
  num_epochs: 50 #测试
  learning_rate: 3.0e-4 #1.0e-4
  betas: [0.9, 0.98]
  eps: 1.0e-9
  weight_decay: 0.01
  warmup_steps: 4000
  gradient_clip: 1.0
  label_smoothing: 0.1
  save_dir: "ablation_models/2"
  log_dir: "ablation/logs"