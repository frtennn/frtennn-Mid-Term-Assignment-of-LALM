model:
  d_model: 128 #512
  n_heads: 4 #8
  num_encoder_layers: 2 #6
  num_decoder_layers: 2 #6
  d_ff: 512 #2048
  dropout: 0.1
  max_seq_length: 256
  vocab_size: 30000

training:
  batch_size: 64
  num_epochs: 50 #5
  learning_rate: 3.0e-4 #1.0e-4
  betas: [0.9, 0.98]
  eps: 1.0e-9
  weight_decay: 0.01
  warmup_steps: 4000
  gradient_clip: 1.0
  label_smoothing: 0.1
  save_dir: "checkpoints"
  log_dir: "logs"